{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 1: Parsing the dataset",
   "id": "25bf1638385dfd11"
  },
  {
   "metadata": {
    "id": "Gd9vXkT0Uf4_",
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:16.392571Z",
     "start_time": "2026-02-10T16:13:16.379117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from spyder.utils.snippets.lexer import tokenize\n",
    "\n",
    "url = \"\"\"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\"\"\n",
    "if not os.path.exists(url.split(\"/\")[-1]):\n",
    "  urlretrieve(url, url.split(\"/\")[-1])\n",
    "  print(\"Downloaded\", url)\n",
    "  with tarfile.open('review_polarity.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "  print(\"Extracted archive\")"
   ],
   "id": "21c0b46e4bd69934",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:16.402224Z",
     "start_time": "2026-02-10T16:13:16.396126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np"
   ],
   "id": "d19df1e5cc010efe",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:16.454102Z",
     "start_time": "2026-02-10T16:13:16.405091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# takes a path and returns a list with the content of all files at that path\n",
    "def get_reviews(path):\n",
    "    filenames_lst = os.listdir(path)\n",
    "    reviews_lst = []\n",
    "    for i in range(len(filenames_lst)):\n",
    "        path_full = os.path.join(path, filenames_lst[i])\n",
    "        with open(path_full, 'r') as f:\n",
    "            reviews_lst.append(f.read())\n",
    "    return reviews_lst\n",
    "\n",
    "folder_name = 'txt_sentoken'\n",
    "neg_path = os.path.join(folder_name, 'neg')\n",
    "pos_path = os.path.join(folder_name, 'pos')\n",
    "neg_reviews = get_reviews(neg_path)\n",
    "pos_reviews = get_reviews(pos_path)"
   ],
   "id": "873c77fb3c8c3633",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:16.457209Z",
     "start_time": "2026-02-10T16:13:16.454455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check that both lists are the correct length\n",
    "assert len(neg_reviews) == 1000\n",
    "assert len(neg_reviews) == len(pos_reviews)"
   ],
   "id": "b5d3125bdb582df8",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:16.461542Z",
     "start_time": "2026-02-10T16:13:16.457414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the full list\n",
    "X_full = neg_reviews + pos_reviews\n",
    "\n",
    "# create the training list and testing list\n",
    "split_point = int(len(neg_reviews)*0.8)\n",
    "X_train = neg_reviews[:split_point] + pos_reviews[:split_point]\n",
    "X_test = neg_reviews[split_point:] + pos_reviews[split_point:]\n",
    "\n",
    "# create the corresponding outcome lists\n",
    "y_full = [-1 for i in range(int(len(X_full)/2))] + [1 for i in range(int(len(X_full)/2))]\n",
    "y_train = [-1 for i in range(split_point)] + [1 for i in range(split_point)]\n",
    "y_test = [-1 for i in range(int(len(X_test)/2))] + [1 for i in range(int(len(X_test)/2))]"
   ],
   "id": "a44dc43c98e22ce6",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:16.465257Z",
     "start_time": "2026-02-10T16:13:16.461744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the train and test lists are the correct lengths\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "\n",
    "# check the train and test lists are the correct types / values\n",
    "assert np.all([isinstance(x, str) for x in X_train])\n",
    "assert np.all([isinstance(x, str) for x in X_test])\n",
    "assert len(np.unique(y_train))==2\n",
    "assert min(y_train) == -1\n",
    "assert max(y_train) == 1\n",
    "assert len(np.unique(y_test))==2\n",
    "assert min(y_test) == -1\n",
    "assert max(y_test) == 1"
   ],
   "id": "53e8a2189e7f5aa3",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2: Feature extraction",
   "id": "c05a8a7fc9c59f9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:16.469032Z",
     "start_time": "2026-02-10T16:13:16.465510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vectorizer:\n",
    "\n",
    "    # the instance will only store the vocabulary once the method fit has been used\n",
    "    def __init__(self):\n",
    "        self.vocabulary = None\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(txt):\n",
    "        return [token for line in txt.split(\"\\n\") for token in line.split()]\n",
    "\n",
    "    # takes a list of texts and creates the vocabulary (list of unique tokens)\n",
    "    def make_vocabulary(self, data):\n",
    "        \"\"\" not efficient: many tokens appear in many reviews so the 2nd for loop will iterate over the same token many times \"\"\"\n",
    "        vocab = []\n",
    "        for review in data:\n",
    "            for token in self.tokenize(review):\n",
    "                if token not in vocab:\n",
    "                    vocab.append(token)\n",
    "        self.vocabulary = vocab\n",
    "\n",
    "    # takes a list of texts and returns its corresponding bag-of-words vector, according to the vocabulary\n",
    "    def transform_to_bow(self, data_list):\n",
    "        matrix_size = (len(data_list), len(self.vocabulary))\n",
    "        X = np.zeros(matrix_size)\n",
    "        for i, txt in enumerate(data_list):\n",
    "            for token in self.tokenize(txt):\n",
    "                X[i, self.vocabulary.index(token)] = 1\n",
    "        return X"
   ],
   "id": "414744c2411f7c3e",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:59.485127Z",
     "start_time": "2026-02-10T16:13:16.469237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer = Vectorizer()\n",
    "\n",
    "# create the vocabulary\n",
    "\"\"\" both train and test sets are included in the vocabulary, to avoid unseen words later \"\"\"\n",
    "vectorizer.make_vocabulary(X_full)\n",
    "\n",
    "# turns the lists into bag_of_words matrices\n",
    "X_train = vectorizer.transform_to_bow(X_train)\n",
    "X_test = vectorizer.transform_to_bow(X_test)\n",
    "\n",
    "# transform the y lists into np arrays for an easier time down the line\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ],
   "id": "71d40e495c717c93",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:59.498692Z",
     "start_time": "2026-02-10T16:13:59.492838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the vocabulary\n",
    "ordered_vocabulary = vectorizer.vocabulary"
   ],
   "id": "6937630297c7295",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:13:59.506278Z",
     "start_time": "2026-02-10T16:13:59.499685Z"
    }
   },
   "cell_type": "code",
   "source": "assert X_test.shape[1] == X_train.shape[1]",
   "id": "45cc1a3e2726ed6c",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3: Learning framework",
   "id": "dcdcb8802b3bf7c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:12:03.884373Z",
     "start_time": "2026-02-10T17:12:03.865561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Classifier:\n",
    "    def __init__(self, vocabulary, n_max_iter=10, regularizer_dampening=0.001, learning_rate=0.1):\n",
    "        self.param_vector = np.random.normal(size=len(vocabulary)+1)\n",
    "        self.n_max_iter = n_max_iter\n",
    "        self.lambduh = regularizer_dampening\n",
    "        self.gamma = learning_rate\n",
    "        self.loss = []\n",
    "\n",
    "    def _score(self, X):\n",
    "        z = np.matmul(X, self.param_vector)\n",
    "        return z\n",
    "\n",
    "    def _predict(self, X):\n",
    "        z = self._score(X)\n",
    "        y_hat = np.sign(z).reshape((z.shape[0],1))\n",
    "        return y_hat\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        \"\"\" l1 or l2? loss sum instead of loss average? intercept also regularised? \"\"\"\n",
    "        z = self._score(X)\n",
    "        loss_vector = np.maximum(0,1-np.multiply(y,z)) # hinge function\n",
    "        regularization = self.lambduh * np.sum(np.absolute(self.param_vector))\n",
    "        loss = regularization + np.sum(loss_vector)\n",
    "        self.loss.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        v1 = np.multiply(y, self._score(X))\n",
    "        index = v1 < 1\n",
    "        v = np.zeros(X.shape)\n",
    "        v[index] = -y[index].reshape((-1,1)) * X[index]\n",
    "        return self.lambduh * np.sign(self.param_vector) + np.sum(v, axis=0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, 1) # add the pseudo-input\n",
    "        i = 0\n",
    "        while i < self.n_max_iter:\n",
    "            self._loss(X, y)\n",
    "            self.param_vector = self.param_vector - self.gamma * self._gradient(X, y)\n",
    "            i += 1\n",
    "        return self"
   ],
   "id": "6c5b8d8af6c2a36",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:12:08.622688Z",
     "start_time": "2026-02-10T17:12:06.779078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Classifier(ordered_vocabulary)\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "4cb34b672cb4f68c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Classifier at 0x169f91550>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 181
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

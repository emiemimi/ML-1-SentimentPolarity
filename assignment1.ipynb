{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 1: Parsing the dataset",
   "id": "25bf1638385dfd11"
  },
  {
   "metadata": {
    "id": "Gd9vXkT0Uf4_",
    "ExecuteTime": {
     "end_time": "2026-01-28T10:07:47.993861Z",
     "start_time": "2026-01-28T10:07:45.080563Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted archive\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import os.path\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"\"\"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\"\"\n",
    "if not os.path.exists(url.split(\"/\")[-1]):\n",
    "  urlretrieve(url, url.split(\"/\")[-1])\n",
    "  print(\"Downloaded\", url)\n",
    "  with tarfile.open('review_polarity.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "  print(\"Extracted archive\")"
   ],
   "id": "21c0b46e4bd69934"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T10:18:34.623191Z",
     "start_time": "2026-01-30T10:18:34.392579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_reviews(path):\n",
    "    \"\"\" takes a path and returns a list with the content of all files at that path \"\"\"\n",
    "    filenames_lst = os.listdir(path)\n",
    "    reviews_lst = []\n",
    "    for i in range(len(filenames_lst)):\n",
    "        path_full = os.path.join(path, filenames_lst[i])\n",
    "        with open(path_full, 'r') as f:\n",
    "            reviews_lst.append(f.read())\n",
    "    return reviews_lst\n",
    "\n",
    "folder_name = 'txt_sentoken'\n",
    "neg_path = os.path.join(folder_name, 'neg')\n",
    "pos_path = os.path.join(folder_name, 'pos')\n",
    "neg_reviews = get_reviews(neg_path)\n",
    "pos_reviews = get_reviews(pos_path)"
   ],
   "id": "873c77fb3c8c3633",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T10:18:44.366781Z",
     "start_time": "2026-01-30T10:18:44.349315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check that both lists are the correct length\n",
    "assert len(neg_reviews) == 1000\n",
    "assert len(neg_reviews) == len(pos_reviews)"
   ],
   "id": "b5d3125bdb582df8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T10:18:47.770437Z",
     "start_time": "2026-01-30T10:18:47.755247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the train and test lists\n",
    "split_point = int(len(neg_reviews)*0.8)\n",
    "X_train = neg_reviews[:split_point] + pos_reviews[:split_point]\n",
    "y_train = [-1 for i in range(split_point)] + [1 for i in range(split_point)]\n",
    "X_test = neg_reviews[split_point:] + pos_reviews[split_point:]\n",
    "y_test = [-1 for i in range(int(len(X_test)/2))] + [1 for i in range(int(len(X_test)/2))]"
   ],
   "id": "a44dc43c98e22ce6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T10:18:52.538524Z",
     "start_time": "2026-01-30T10:18:52.521916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the train and test lists are the correct lengths\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "\n",
    "# check the train and test lists are the correct types / values\n",
    "assert np.all([isinstance(x, str) for x in X_train])\n",
    "assert np.all([isinstance(x, str) for x in X_test])\n",
    "assert len(np.unique(y_train))==2\n",
    "assert min(y_train) == -1\n",
    "assert max(y_train) == 1\n",
    "assert len(np.unique(y_test))==2\n",
    "assert min(y_test) == -1\n",
    "assert max(y_test) == 1"
   ],
   "id": "53e8a2189e7f5aa3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2: Feature extraction",
   "id": "c05a8a7fc9c59f9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:41:35.580118Z",
     "start_time": "2026-01-30T13:41:35.479486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the list of all unique tokens from all the reviews\n",
    "# I include both train and test sets for building the vocabulary, in order to avoid having to handle unseen words from the test set\n",
    "\n",
    "def get_token_lst(txt):\n",
    "    tokens = []\n",
    "    for line in txt.split('\\n'):\n",
    "        for token in line.split(' '):\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "X_full = X_train + X_test\n",
    "vocab_set = set()\n",
    "for review in X_full:\n",
    "    vocab_set.update(get_token_lst(review))\n",
    "vocab = list(vocab_set)"
   ],
   "id": "f9d0590ae84d9410",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:58:59.983732Z",
     "start_time": "2026-01-30T13:58:59.972947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get a binary bag-of-words matrix from a dataset\n",
    "def get_bog(x_in, vocabulary):\n",
    "    x_out = np.zeros((len(x_in), len(vocabulary)))\n",
    "    for i, txt in enumerate(x_in):\n",
    "        tokens = list(set(get_token_lst(txt)))\n",
    "        for token in tokens:\n",
    "            x_out[i, vocab.index(token)] = 1\n",
    "    return x_out"
   ],
   "id": "7f605be85b3f4e34",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:01:03.736174Z",
     "start_time": "2026-01-30T13:59:30.753141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_bog = get_bog(X_train, vocab)\n",
    "X_test_bog = get_bog(X_test, vocab)"
   ],
   "id": "61d12d38beecbccc",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        pass"
   ],
   "id": "fc9ad252d4c2fde6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3: Learning framework",
   "id": "dcdcb8802b3bf7c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Classifier:\n",
    "    def __init(self):\n",
    "        pass"
   ],
   "id": "6c5b8d8af6c2a36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 1: Parsing the dataset",
   "id": "25bf1638385dfd11"
  },
  {
   "metadata": {
    "id": "Gd9vXkT0Uf4_",
    "ExecuteTime": {
     "end_time": "2026-02-07T18:01:11.943873Z",
     "start_time": "2026-02-07T18:01:09.974565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from spyder.utils.snippets.lexer import tokenize\n",
    "\n",
    "url = \"\"\"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\"\"\n",
    "if not os.path.exists(url.split(\"/\")[-1]):\n",
    "  urlretrieve(url, url.split(\"/\")[-1])\n",
    "  print(\"Downloaded\", url)\n",
    "  with tarfile.open('review_polarity.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "  print(\"Extracted archive\")"
   ],
   "id": "21c0b46e4bd69934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/z6_d10jx0vd7ml1r35x7kfkm0000gn/T/ipykernel_47748/3499532314.py:10: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted archive\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T18:05:29.748552Z",
     "start_time": "2026-02-07T18:05:29.564173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# takes a path and returns a list with the content of all files at that path\n",
    "def get_reviews(path):\n",
    "    filenames_lst = os.listdir(path)\n",
    "    reviews_lst = []\n",
    "    for i in range(len(filenames_lst)):\n",
    "        path_full = os.path.join(path, filenames_lst[i])\n",
    "        with open(path_full, 'r') as f:\n",
    "            reviews_lst.append(f.read())\n",
    "    return reviews_lst\n",
    "\n",
    "folder_name = 'txt_sentoken'\n",
    "neg_path = os.path.join(folder_name, 'neg')\n",
    "pos_path = os.path.join(folder_name, 'pos')\n",
    "neg_reviews = get_reviews(neg_path)\n",
    "pos_reviews = get_reviews(pos_path)"
   ],
   "id": "873c77fb3c8c3633",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T18:06:33.430942Z",
     "start_time": "2026-02-07T18:06:33.392021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check that both lists are the correct length\n",
    "assert len(neg_reviews) == 1000\n",
    "assert len(neg_reviews) == len(pos_reviews)"
   ],
   "id": "b5d3125bdb582df8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T18:20:36.129741Z",
     "start_time": "2026-02-07T18:20:36.099880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the full list\n",
    "X_full = neg_reviews + pos_reviews\n",
    "\n",
    "# create the training list and testing list\n",
    "split_point = int(len(neg_reviews)*0.8)\n",
    "X_train = neg_reviews[:split_point] + pos_reviews[:split_point]\n",
    "X_test = neg_reviews[split_point:] + pos_reviews[split_point:]\n",
    "\n",
    "# create the corresponding outcome lists\n",
    "y_full = [-1 for i in range(int(len(X_full)/2))] + [1 for i in range(int(len(X_full)/2))]\n",
    "y_train = [-1 for i in range(split_point)] + [1 for i in range(split_point)]\n",
    "y_test = [-1 for i in range(int(len(X_test)/2))] + [1 for i in range(int(len(X_test)/2))]"
   ],
   "id": "a44dc43c98e22ce6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T18:20:45.958295Z",
     "start_time": "2026-02-07T18:20:45.931008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the train and test lists are the correct lengths\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "\n",
    "# check the train and test lists are the correct types / values\n",
    "assert np.all([isinstance(x, str) for x in X_train])\n",
    "assert np.all([isinstance(x, str) for x in X_test])\n",
    "assert len(np.unique(y_train))==2\n",
    "assert min(y_train) == -1\n",
    "assert max(y_train) == 1\n",
    "assert len(np.unique(y_test))==2\n",
    "assert min(y_test) == -1\n",
    "assert max(y_test) == 1"
   ],
   "id": "53e8a2189e7f5aa3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2: Feature extraction",
   "id": "c05a8a7fc9c59f9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T20:01:44.027847Z",
     "start_time": "2026-02-07T20:01:43.995870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vectorizer:\n",
    "\n",
    "    # the instance will only store the vocabulary once the method fit has been used\n",
    "    def __init__(self):\n",
    "        self.vocabulary = None\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(txt):\n",
    "        return [token for line in txt.split(\"\\n\") for token in line.split()]\n",
    "\n",
    "    # takes a list of texts and creates the vocabulary (list of unique tokens)\n",
    "    def fit(self, data):\n",
    "        \"\"\" not efficient: many tokens appear in many reviews so the 2nd for loop will iterate over the same token many times \"\"\"\n",
    "        vocab = []\n",
    "        for review in data:\n",
    "            for token in self.tokenize(review):\n",
    "                if token not in vocab:\n",
    "                    vocab.append(token)\n",
    "        self.vocabulary = vocab\n",
    "\n",
    "    # takes a list of texts and returns its corresponding bag-of-words vector, according to the vocabulary\n",
    "    def transform_to_bow(self, data_list):\n",
    "        matrix_size = (len(data_list), len(self.vocabulary))\n",
    "        X = np.zeros(matrix_size)\n",
    "        for i, txt in enumerate(data_list):\n",
    "            for token in self.tokenize(txt):\n",
    "                X[i, self.vocabulary.index(token)] = 1\n",
    "        return X"
   ],
   "id": "414744c2411f7c3e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T20:02:32.987451Z",
     "start_time": "2026-02-07T20:01:48.732680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer = Vectorizer()\n",
    "\n",
    "# create the vocabulary\n",
    "\"\"\" both train and test sets are included in the vocabulary, to avoid unseen words later \"\"\"\n",
    "vectorizer.fit(X_full)\n",
    "\n",
    "# turns the lists into bag_of_words matrices\n",
    "X_train = vectorizer.transform_to_bow(X_train)\n",
    "X_test = vectorizer.transform_to_bow(X_test)"
   ],
   "id": "71d40e495c717c93",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T20:11:07.163046Z",
     "start_time": "2026-02-07T20:11:07.114426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the vocabulary\n",
    "ordered_vocabulary = vectorizer.vocabulary"
   ],
   "id": "6937630297c7295",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T20:11:11.864027Z",
     "start_time": "2026-02-07T20:11:11.841961Z"
    }
   },
   "cell_type": "code",
   "source": "assert X_test.shape[1] == X_train.shape[1]",
   "id": "45cc1a3e2726ed6c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3: Learning framework",
   "id": "dcdcb8802b3bf7c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Classifier:\n",
    "    def __init(self):\n",
    "        pass"
   ],
   "id": "6c5b8d8af6c2a36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
